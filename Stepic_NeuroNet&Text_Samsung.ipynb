{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Задачи к курсу на Stepic \"Нейронные сети и обработка текста\"\n",
    " https://stepik.org/course/54098\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T06:19:36.891292Z",
     "start_time": "2019-10-23T06:19:36.880292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6083 0.1521 0.0676 0.038 0.0243 0.0169 0.0124 0.0095 0.0075 0.0061 0.005 0.0042 0.0036 0.0031 0.0027 0.0024 0.0021 0.0019 0.0017 0.0015 0.0014 0.0013 0.0011 0.0011 0.001 0.0009 0.0008 0.0008 0.0007 0.0007 0.0006 0.0006 0.0006 0.0005 0.0005\n",
      "976\n",
      "0.9999999999999989\n"
     ]
    }
   ],
   "source": [
    "#2.1 Векторная модель текста и TF-IDF. Exercise 2\n",
    "#Закон Ципфа\n",
    "\n",
    "N=1000\n",
    "s=2\n",
    "Z=sum([i**(-s) for i in range(1,N+1)])\n",
    "print(*[round(1/(rank**s*Z),4) for rank in range(1,N+1)][:35])\n",
    "ver=[round(1/(rank**s*Z),5) for rank in range(1,N+1)]\n",
    "ver10 = list(filter(lambda x: x>0.001, ver))\n",
    "print(1000-len(ver10))\n",
    "print(sum([1/(rank**s*Z) for rank in range(1,N+1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T20:38:01.113775Z",
     "start_time": "2019-10-22T20:38:01.107794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.001\n",
      "8.965784284662087\n",
      "[0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125, 0.0009765625, 0.00048828125, 0.000244140625, 0.0001220703125, 6.103515625e-05, 3.0517578125e-05, 1.52587890625e-05, 7.62939453125e-06, 3.814697265625e-06, 1.9073486328125e-06, 9.5367431640625e-07, 4.76837158203125e-07, 2.384185791015625e-07, 1.1920928955078125e-07, 5.960464477539063e-08, 2.9802322387695312e-08, 1.4901161193847656e-08, 7.450580596923828e-09, 3.725290298461914e-09, 1.862645149230957e-09]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "q=0.5\n",
    "b1=1*(1-q)/(1-q**1000);\n",
    "print(b1)\n",
    "ver10=10/10000\n",
    "print(ver10)\n",
    "m= math.log(ver10/b1)/math.log(q)\n",
    "print(m)\n",
    "print([b1*q**(n-1) for n in range(1,30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T06:21:04.407842Z",
     "start_time": "2019-10-23T06:20:41.004041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gy\n",
      "C:\\Users\\gy\\Documents\\GitHub\\stepik-dl-nlp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\gy\\Documents\\GitHub\\stepik-dl-nlp')\n",
    "print(os.getcwd())\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "    vectorize_texts, SparseFeaturesDataset\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T06:21:04.419825Z",
     "start_time": "2019-10-23T06:21:04.409829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********* Result ******** \n",
      " наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "#2.4 Семинар: классификация новостных текстов. Exercise 2\n",
    "#Постройте словарь (отображение из строкового представления токенов в их номера) и вектор весов (DF).\n",
    "\n",
    "ttt=['Казнить нельзя, помиловать. Нельзя наказывать.','Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "'Нельзя не помиловать.','Обязательно освободить.' ]\n",
    "tokens= tokenize_corpus(ttt, min_token_size=1)\n",
    "#print(tokens)\n",
    "vocabulary, word_doc_freq = build_vocabulary(tokens, min_count=0)\n",
    "#print(vocabulary)\n",
    "\n",
    "\n",
    "vocab2=list(zip(vocabulary.keys(), word_doc_freq))\n",
    "vocab2=sorted(vocab2, key=lambda word: (word[1], word[0]) )\n",
    "print('\\n','********* Result ********','\\n',*[voc[0] for voc in vocab2])\n",
    "print(*[voc[1] for voc in vocab2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:54:51.811836Z",
     "start_time": "2019-10-23T12:54:51.738780Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorize_texts_plus(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True, debug_print=False):\n",
    "    assert mode in {'tfidf2', 'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix(\n",
    "        (len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "        # разделить каждый столбец на вес слова\n",
    "        result = result.multiply(1 / word2freq)\n",
    "\n",
    "    elif mode == \"tfidf2\":\n",
    "        result = result.tocsr()\n",
    "        # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / result.sum(1))  \n",
    "        if debug_print:\n",
    "            print('TF')\n",
    "            print(result)\n",
    "        result = result + np.ones((len(tokenized_texts), len(word2id)))\n",
    "        if debug_print:\n",
    "            print('TF+1')\n",
    "            print(result)\n",
    "        \n",
    "        result = scipy.sparse.dok_matrix(np.log(result))\n",
    "        if debug_print:\n",
    "            print('log(TF+1)')\n",
    "            print(result)\n",
    "\n",
    "            # разделить каждый столбец на вес слова\n",
    "\n",
    "            print('разделить каждый столбец на вес слова')\n",
    "        result = result.multiply(1 / word2freq)\n",
    "        if debug_print:\n",
    "            print(result)\n",
    "            print('*********')\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:57:52.253298Z",
     "start_time": "2019-10-23T12:57:52.079294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.24309541961478315\n",
      "  (0, 1)\t0.4486296678751223\n",
      "  (0, 2)\t0.36464311855496295\n",
      "  (0, 4)\t0.7292862371099259\n",
      "  (1, 0)\t0.24309541961478315\n",
      "  (1, 1)\t0.4486296678751223\n",
      "  (1, 2)\t0.36464311855496295\n",
      "  (1, 3)\t0.36464311855496295\n",
      "  (2, 0)\t0.3835761179679408\n",
      "  (2, 1)\t0.3835761179679408\n",
      "  (2, 5)\t1.150728319609446\n",
      "  (3, 3)\t0.8109302162163288\n",
      "  (3, 6)\t1.6218604324326575\n",
      "4\n",
      "!!! [[0.24309542 0.44862967 0.36464312 0.         0.72928624 0.\n",
      "  0.        ]]\n",
      "!!! [[0.24309542 0.44862967 0.36464312 0.36464312 0.         0.\n",
      "  0.        ]]\n",
      "!!! [[0.38357612 0.38357612 0.         0.         0.         1.15072832\n",
      "  0.        ]]\n",
      "!!! [[0.         0.         0.         0.81093022 0.         0.\n",
      "  1.62186043]]\n",
      "*****\n",
      "[[0.24309542 0.44862967 0.36464312 0.         0.72928624 0.\n",
      "  0.        ]\n",
      " [0.24309542 0.44862967 0.36464312 0.36464312 0.         0.\n",
      "  0.        ]\n",
      " [0.38357612 0.38357612 0.         0.         0.         1.15072832\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.81093022 0.         0.\n",
      "  1.62186043]]\n",
      "*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:130: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "#2.4 Семинар: классификация новостных текстов. Exercise 3\n",
    "#Постройте матрицу признаков для текстов с шага 5 с использованием словаря и вектора весов, полученного на шаге 5.\n",
    "\n",
    "\n",
    "VECTORIZATION_MODE = 'tfidf2'\n",
    "vect = vectorize_texts_plus(tokens, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE, scale=False)\n",
    "print(vect)\n",
    "nump_vect = vect + np.zeros((len(tokens), len(vocabulary)))\n",
    "#print(nump_vect)\n",
    "print(len(nump_vect))\n",
    "for i in range(len(nump_vect)):\n",
    "    print('!!!',nump_vect[i])\n",
    "    np.std(nump_vect[i], 0, ddof=1)\n",
    "nump_vect_std = nump_vect.std(0, ddof=1)\n",
    "print('*****')\n",
    "print(nump_vect)\n",
    "print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:14:58.523829Z",
     "start_time": "2019-10-23T12:14:58.514832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.5108256237659907\n",
      "  (0, 2)\t2.0709620553277333\n",
      "  (0, 5)\t1.2231435513142097\n",
      "  (0, 1)\t1.916290731874155\n",
      "  (1, 0)\t1.5108256237659907\n",
      "  (1, 2)\t2.0709620553277333\n",
      "  (1, 5)\t1.2231435513142097\n",
      "  (1, 4)\t1.5108256237659907\n",
      "  (2, 2)\t1.2231435513142097\n",
      "  (2, 5)\t1.2231435513142097\n",
      "  (3, 4)\t1.5108256237659907\n",
      "  (3, 3)\t1.916290731874155\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
      "                min_df=0, ngram_range=(1, 1), norm=None, preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize_text_simple_regex at 0x0000025F09EFA268>,\n",
      "                use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "MAX_DF=0.8\n",
    "MIN_COUNT=0\n",
    "\n",
    "sc_vector=TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
    "                                                      max_df=MAX_DF,\n",
    "                                                      min_df=MIN_COUNT, norm=None, sublinear_tf=True, smooth_idf=True)\n",
    "sc_vect_val=sc_vector.fit_transform(ttt)\n",
    "print(sc_vect_val)\n",
    "print(sc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T08:15:23.692333Z",
     "start_time": "2019-10-23T08:15:23.687313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.zeros((len(tokens), len(vocabulary)))\n",
    "feats_std = feature_matrix.std(0, ddof=1)\n",
    "print(feats_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T09:29:56.109108Z",
     "start_time": "2019-10-23T09:29:56.080109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814.697265625 57.220458984375 3757.476806640625\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Семинар: классификация новостных текстов Exercise 5\n",
    "# цените количество памяти, которое экономится при использовании разреженной матрицы\n",
    "\n",
    "doc_n=100000\n",
    "dict_n=10000\n",
    "k_fill=0.5/100\n",
    "\n",
    "full_mb=doc_n*dict_n*4/(1024**2)\n",
    "spare_mb=doc_n*dict_n*12*k_fill/(1024**2)\n",
    "print(full_mb, spare_mb, full_mb-spare_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Свёрточные нейросети для обработки текстов Exercise 2\n",
    "# Примените свёртку с ядром (−0.5,0,0.5) к сигналу (1,1,2,3,3,3,2,1,1). \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vect = [1,1,2,3,3,3,2,1,1]\n",
    "svert = np.array([-0.5,0,0.5])\n",
    "\n",
    "for i in range(len(vect)-2):\n",
    "    a=np.array(vect[i:i+3])\n",
    "    print(np.dot(a,svert), end= ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
